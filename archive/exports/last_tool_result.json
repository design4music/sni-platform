{
  "ok": true,
  "relpath": "etl_pipeline/core/database/models.py",
  "bytes": 35244,
  "truncated": false,
  "content": "\"\"\"\nDatabase models for Strategic Narrative Intelligence ETL Pipeline\n\nThis module defines SQLAlchemy models for storing news articles, feeds,\nprocessing metadata, and vector embeddings using pgvector.\n\"\"\"\n\nimport uuid\nfrom datetime import datetime\nfrom enum import Enum as PyEnum\nfrom typing import Any, Dict, List, Optional\n\nimport structlog\nfrom pgvector.sqlalchemy import Vector\nfrom sqlalchemy import (JSON, Boolean, CheckConstraint, Column, DateTime,\n                        Float, ForeignKey, Index, Integer, String, Text,\n                        UniqueConstraint)\nfrom sqlalchemy.dialects.postgresql import ENUM, JSONB, UUID\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import Session, relationship\n\nlogger = structlog.get_logger(__name__)\n\nBase = declarative_base()\n\n\nclass ProcessingStatus(PyEnum):\n    \"\"\"Processing status enumeration\"\"\"\n\n    PENDING = \"pending\"\n    IN_PROGRESS = \"in_progress\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    FILTERED_OUT = \"filtered_out\"\n    DUPLICATE = \"duplicate\"\n\n\nclass FeedType(PyEnum):\n    \"\"\"Feed type enumeration\"\"\"\n\n    RSS = \"rss\"\n    API = \"api\"\n    SCRAPER = \"scraper\"\n\n\nclass ContentCategory(PyEnum):\n    \"\"\"Content category enumeration\"\"\"\n\n    GEOPOLITICS = \"geopolitics\"\n    MILITARY = \"military\"\n    ENERGY = \"energy\"\n    AI_TECHNOLOGY = \"ai_technology\"\n    ECONOMICS = \"economics\"\n    DIPLOMACY = \"diplomacy\"\n    SECURITY = \"security\"\n    OTHER = \"other\"\n\n\nclass LanguageCode(PyEnum):\n    \"\"\"Supported language codes\"\"\"\n\n    EN = \"en\"\n    RU = \"ru\"\n    DE = \"de\"\n    FR = \"fr\"\n\n\n# Create ENUMs for PostgreSQL\nprocessing_status_enum = ENUM(ProcessingStatus, name=\"processing_status\")\nfeed_type_enum = ENUM(FeedType, name=\"feed_type\")\ncontent_category_enum = ENUM(ContentCategory, name=\"content_category\")\nlanguage_code_enum = ENUM(LanguageCode, name=\"language_code\")\n\n\nclass NewsFeed(Base):\n    \"\"\"\n    News feed configuration and metadata\n    \"\"\"\n\n    __tablename__ = \"news_feeds\"\n\n    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n    name = Column(String(255), nullable=False)\n    url = Column(Text, nullable=False)\n    feed_type = Column(feed_type_enum, nullable=False)\n    language = Column(language_code_enum, nullable=False)\n    country_code = Column(String(2))  # ISO country code\n\n    # Configuration\n    is_active = Column(Boolean, default=True, nullable=False)\n    priority = Column(Integer, default=1)  # 1=high, 5=low\n    fetch_interval_minutes = Column(Integer, default=60)\n\n    # API-specific configuration\n    api_key_required = Column(Boolean, default=False)\n    api_headers = Column(JSON)\n    api_params = Column(JSON)\n\n    # Processing configuration\n    content_xpath = Column(String(500))  # For scraper feeds\n    title_xpath = Column(String(500))\n    date_xpath = Column(String(500))\n\n    # Quality metrics\n    reliability_score = Column(Float, default=0.5)  # 0.0 to 1.0\n    avg_articles_per_day = Column(Float)\n\n    # Timestamps\n    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    last_fetched_at = Column(DateTime)\n    last_successful_fetch_at = Column(DateTime)\n\n    # Relationships\n    articles = relationship(\n        \"Article\", back_populates=\"feed\", cascade=\"all, delete-orphan\"\n    )\n    feed_metrics = relationship(\n        \"FeedMetrics\", back_populates=\"feed\", cascade=\"all, delete-orphan\"\n    )\n\n    __table_args__ = (\n        Index(\"idx_news_feeds_active_priority\", \"is_active\", \"priority\"),\n        Index(\"idx_news_feeds_language\", \"language\"),\n        Index(\"idx_news_feeds_last_fetched\", \"last_fetched_at\"),\n    )\n\n\nclass Article(Base):\n    \"\"\"\n    News article with full content and metadata\n    \"\"\"\n\n    __tablename__ = \"articles\"\n\n    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n    feed_id = Column(UUID(as_uuid=True), ForeignKey(\"news_feeds.id\"), nullable=False)\n\n    # Content\n    title = Column(Text, nullable=False)\n    content = Column(Text)\n    summary = Column(Text)\n    url = Column(Text, nullable=False)\n    author = Column(String(255))\n\n    # Metadata\n    published_at = Column(DateTime, nullable=False)\n    language = Column(language_code_enum, nullable=False)\n    source_name = Column(String(255))\n\n    # Content hashing for deduplication\n    content_hash = Column(String(64), nullable=False)  # SHA-256 hash\n    title_hash = Column(String(64), nullable=False)\n\n    # Processing status\n    processing_status = Column(processing_status_enum, default=ProcessingStatus.PENDING)\n    ingestion_status = Column(processing_status_enum, default=ProcessingStatus.PENDING)\n    filtering_status = Column(processing_status_enum, default=ProcessingStatus.PENDING)\n    ner_status = Column(processing_status_enum, default=ProcessingStatus.PENDING)\n    ml_status = Column(processing_status_enum, default=ProcessingStatus.PENDING)\n\n    # Quality metrics\n    relevance_score = Column(Float)  # 0.0 to 1.0\n    quality_score = Column(Float)  # 0.0 to 1.0\n    sentiment_score = Column(Float)  # -1.0 to 1.0\n\n    # Content analysis\n    word_count = Column(Integer)\n    reading_time_minutes = Column(Float)\n\n    # Categorization\n    primary_category = Column(content_category_enum)\n    categories = Column(JSON)  # List of categories with confidence scores\n\n    # Geographic information\n    countries_mentioned = Column(JSON)  # List of country codes\n    regions_mentioned = Column(JSON)  # List of geographic regions\n\n    # Timestamps\n    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    processed_at = Column(DateTime)\n\n    # Relationships\n    feed = relationship(\"NewsFeed\", back_populates=\"articles\")\n    entities = relationship(\n        \"EntityMention\", back_populates=\"article\", cascade=\"all, delete-orphan\"\n    )\n    embeddings = relationship(\n        \"ArticleEmbedding\", back_populates=\"article\", cascade=\"all, delete-orphan\"\n    )\n    clusters = relationship(\"ArticleCluster\", back_populates=\"article\")\n\n    __table_args__ = (\n        UniqueConstraint(\"feed_id\", \"content_hash\", name=\"uq_article_content_hash\"),\n        Index(\"idx_articles_published_at\", \"published_at\"),\n        Index(\"idx_articles_processing_status\", \"processing_status\"),\n        Index(\"idx_articles_language\", \"language\"),\n        Index(\"idx_articles_category\", \"primary_category\"),\n        Index(\"idx_articles_relevance\", \"relevance_score\"),\n        Index(\"idx_articles_url_hash\", \"content_hash\"),\n        CheckConstraint(\"relevance_score >= 0 AND relevance_score <= 1\"),\n        CheckConstraint(\"quality_score >= 0 AND quality_score <= 1\"),\n        CheckConstraint(\"sentiment_score >= -1 AND sentiment_score <= 1\"),\n    )\n\n\nclass EntityMention(Base):\n    \"\"\"\n    Named Entity Recognition results for articles\n    \"\"\"\n\n    __tablename__ = \"entity_mentions\"\n\n    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n    article_id = Column(UUID(as_uuid=True), ForeignKey(\"articles.id\"), nullable=False)\n\n    # Entity details\n    entity_text = Column(String(500), nullable=False)\n    entity_type = Column(String(50), nullable=False)  # PERSON, ORG, GPE, etc.\n    entity_label = Column(String(200))  # Normalized entity name\n\n    # Position in text\n    start_char = Column(Integer, nullable=False)\n    end_char = Column(Integer, nullable=False)\n\n    # Confidence and context\n    confidence_score = Column(Float, nullable=False)\n    context_snippet = Column(Text)\n\n    # Normalization and linking\n    knowledge_base_id = Column(String(100))  # Link to external KB\n    wikipedia_id = Column(String(100))\n    coordinates = Column(JSON)  # For geographic entities\n\n    # Timestamps\n    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n\n    # Relationships\n    article = relationship(\"Article\", back_populates=\"entities\")\n\n    __table_args__ = (\n        Index(\"idx_entity_mentions_article\", \"article_id\"),\n        Index(\"idx_entity_mentions_type\", \"entity_type\"),\n        Index(\"idx_entity_mentions_label\", \"entity_label\"),\n        Index(\"idx_entity_mentions_confidence\", \"confidence_score\"),\n    )\n\n\nclass ArticleEmbedding(Base):\n    \"\"\"\n    Vector embeddings for articles using pgvector\n    \"\"\"\n\n    __tablename__ = \"article_embeddings\"\n\n    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n    article_id = Column(UUID(as_uuid=True), ForeignKey(\"articles.id\"), nullable=False)\n\n    # Embedding details\n    embedding_model = Column(\n        String(100), nullable=False\n    )  # e.g., 'sentence-transformers/all-MiniLM-L6-v2'\n    embedding_version = Column(String(20), nullable=False)\n\n    # Vector embeddings (pgvector)\n    title_embedding = Column(Vector(384))  # Adjust dimension based on model\n    content_embedding = Column(Vector(384))\n    summary_embedding = Column(Vector(384))\n\n    # Metadata\n    embedding_created_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n\n    # Relationships\n    article = relationship(\"Article\", back_populates=\"embeddings\")\n\n    __table_args__ = (\n        UniqueConstraint(\n            \"article_id\",\n            \"embedding_model\",\n            \"embedding_version\",\n            name=\"uq_article_embedding\",\n        ),\n        Index(\"idx_article_embeddings_model\", \"embedding_model\"),\n    )\n\n\nclass ArticleCluster(Base):\n    \"\"\"\n    ML clustering results for articles\n    \"\"\"\n\n    __tablename__ = \"article_clusters\"\n\n    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n    article_id = Column(UUID(as_uuid=True), ForeignKey(\"articles.id\"), nullable=False)\n\n    # Cluster information\n    cluster_id = Column(String(100), nullable=False)\n    cluster_algorithm = Column(String(50), nullable=False)  # e.g., 'CLUST-1', 'CLUST-2'\n    cluster_version = Column(String(20), nullable=False)\n\n    # Clustering metadata\n    similarity_score = Column(Float)  # Similarity to cluster centroid\n    cluster_size = Column(Integer)  # Number of articles in cluster\n    cluster_label = Column(String(255))  # Human-readable cluster label\n    cluster_keywords = Column(JSON)  # Key terms/topics for cluster\n\n    # Timestamps\n    clustered_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n\n    # Relationships\n    article = relationship(\"Article\", back_populates=\"clusters\")\n\n    __table_args__ = (\n        Index(\"idx_article_clusters_cluster_id\", \"cluster_id\"),\n        Index(\"idx_article_clusters_algorithm\", \"cluster_algorithm\"),\n        Index(\"idx_article_clusters_similarity\", \"similarity_score\"),\n    )\n\n\nclass TrendingTopic(Base):\n    \"\"\"\n    Real-time trending topics and narratives\n    \"\"\"\n\n    __tablename__ = \"trending_topics\"\n\n    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n\n    # Topic details\n    topic_name = Column(String(255), nullable=False)\n    topic_keywords = Column(JSON, nullable=False)  # List of keywords\n    topic_description = Column(Text)\n\n    # Trending metrics\n    mention_count = Column(Integer, default=0)\n    trending_score = Column(Float, nullable=False)  # Calculated trending intensity\n    velocity = Column(Float)  # Rate of change in mentions\n\n    # Time window\n    window_start = Column(DateTime, nullable=False)\n    window_end = Column(DateTime, nullable=False)\n\n    # Geographic distribution\n    countries_trending = Column(JSON)  # List of country codes where trending\n    languages = Column(JSON)  # Languages where topic is trending\n\n    # Related articles\n    article_count = Column(Integer, default=0)\n    sample_article_ids = Column(JSON)  # Sample of related article IDs\n\n    # Timestamps\n    detected_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n    last_updated_at = Column(\n        DateTime, default=datetime.utcnow, onupdate=datetime.utcnow\n    )\n\n    __table_args__ = (\n        Index(\"idx_trending_topics_score\", \"trending_score\"),\n        Index(\"idx_trending_topics_detected\", \"detected_at\"),\n        Index(\"idx_trending_topics_window\", \"window_start\", \"window_end\"),\n    )\n\n\nclass PipelineRun(Base):\n    \"\"\"\n    ETL pipeline execution tracking\n    \"\"\"\n\n    __tablename__ = \"pipeline_runs\"\n\n    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n    pipeline_id = Column(String(100), nullable=False)\n\n    # Execution details\n    status = Column(processing_status_enum, nullable=False)\n    started_at = Column(DateTime, nullable=False)\n    completed_at = Column(DateTime)\n\n    # Metrics\n    feeds_processed = Column(Integer, default=0)\n    articles_ingested = Column(Integer, default=0)\n    articles_filtered = Column(Integer, default=0)\n    articles_processed = Column(Integer, default=0)\n    errors_count = Column(Integer, default=0)\n\n    # Performance metrics\n    processing_time_seconds = Column(Float)\n    throughput_articles_per_second = Column(Float)\n\n    # Configuration snapshot\n    config_snapshot = Column(JSON)\n\n    # Error details\n    error_message = Column(Text)\n    error_details = Column(JSON)\n\n    __table_args__ = (\n        Index(\"idx_pipeline_runs_pipeline_id\", \"pipeline_id\"),\n        Index(\"idx_pipeline_runs_status\", \"status\"),\n        Index(\"idx_pipeline_runs_started\", \"started_at\"),\n    )\n\n\nclass FeedMetrics(Base):\n    \"\"\"\n    Feed performance and quality metrics\n    \"\"\"\n\n    __tablename__ = \"feed_metrics\"\n\n    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n    feed_id = Column(UUID(as_uuid=True), ForeignKey(\"news_feeds.id\"), nullable=False)\n\n    # Time window\n    date = Column(DateTime, nullable=False)  # Daily metrics\n\n    # Fetch metrics\n    fetch_attempts = Column(Integer, default=0)\n    fetch_successes = Column(Integer, default=0)\n    fetch_failures = Column(Integer, default=0)\n\n    # Content metrics\n    articles_fetched = Column(Integer, default=0)\n    articles_new = Column(Integer, default=0)\n    articles_duplicate = Column(Integer, default=0)\n    articles_filtered_out = Column(Integer, default=0)\n\n    # Quality metrics\n    avg_relevance_score = Column(Float)\n    avg_quality_score = Column(Float)\n    avg_processing_time_seconds = Column(Float)\n\n    # Error tracking\n    error_count = Column(Integer, default=0)\n    last_error_message = Column(Text)\n\n    # Relationships\n    feed = relationship(\"NewsFeed\", back_populates=\"feed_metrics\")\n\n    __table_args__ = (\n        UniqueConstraint(\"feed_id\", \"date\", name=\"uq_feed_metrics_daily\"),\n        Index(\"idx_feed_metrics_date\", \"date\"),\n        Index(\n            \"idx_feed_metrics_fetch_success_rate\", \"fetch_successes\", \"fetch_attempts\"\n        ),\n    )\n\n\nclass DataQualityReport(Base):\n    \"\"\"\n    Data quality assessment reports\n    \"\"\"\n\n    __tablename__ = \"data_quality_reports\"\n\n    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n\n    # Report details\n    report_type = Column(String(50), nullable=False)  # 'daily', 'weekly', 'monthly'\n    report_date = Column(DateTime, nullable=False)\n\n    # Quality metrics\n    total_articles = Column(Integer, default=0)\n    duplicate_articles = Column(Integer, default=0)\n    low_quality_articles = Column(Integer, default=0)\n    irrelevant_articles = Column(Integer, default=0)\n\n    # Language distribution\n    language_distribution = Column(JSON)\n\n    # Category distribution\n    category_distribution = Column(JSON)\n\n    # Feed performance summary\n    active_feeds = Column(Integer, default=0)\n    failing_feeds = Column(Integer, default=0)\n    avg_feed_reliability = Column(Float)\n\n    # Processing performance\n    avg_processing_time = Column(Float)\n    processing_error_rate = Column(Float)\n\n    # Data freshness\n    avg_article_age_hours = Column(Float)\n    stale_articles_count = Column(Integer, default=0)\n\n    # Detailed analysis\n    quality_issues = Column(JSON)  # Detailed quality issue breakdown\n    recommendations = Column(JSON)  # System recommendations\n\n    # Timestamps\n    generated_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n\n    __table_args__ = (\n        Index(\"idx_data_quality_reports_date\", \"report_date\"),\n        Index(\"idx_data_quality_reports_type\", \"report_type\"),\n    )\n\n\n# ============================================================================\n# NSF-1 NARRATIVE INTELLIGENCE MODELS\n# ============================================================================\n\n\nclass NarrativeNSF1(Base):\n    \"\"\"\n    NSF-1 Narrative Model - Exact match to finalized specification\n    Uses UUID primary key internally with narrative_id for display/API\n    This is the main Narrative model for content storage.\n    \"\"\"\n\n    __tablename__ = \"narratives\"\n\n    # PRIMARY KEY - UUID for internal use\n    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n\n    # DISPLAY ID - Used by API and frontend (e.g., \"EN-002-A\")\n    narrative_id = Column(String(50), nullable=False, unique=True)\n\n    # CORE NSF-1 FIELDS (scalar columns)\n    title = Column(String(500), nullable=False)\n    summary = Column(Text, nullable=False)\n    origin_language = Column(String(2), nullable=False)\n\n    # QUALITY AND CONFIDENCE (scalar columns)\n    confidence_rating = Column(String(20))  # low, medium, high, very_high\n\n    # FRINGE AND QUALITY METADATA - structured JSONB fields\n    fringe_notes = Column(\n        JSONB, nullable=False, default=list\n    )  # Array of fringe/outlier annotations\n    data_quality_notes = Column(\n        JSONB, nullable=False, default=list\n    )  # Array of data quality issues\n\n    # HIERARCHY FIELD - canonical parent/child relationship\n    parent_id = Column(\n        UUID(as_uuid=True),\n        ForeignKey(\"narratives.id\", ondelete=\"CASCADE\"),\n        nullable=True,\n        index=True,\n    )\n\n    # ARRAY FIELDS - stored as JSONB\n    dominant_source_languages = Column(JSONB, nullable=False, default=list)\n    alignment = Column(JSONB, nullable=False, default=list)\n    actor_origin = Column(JSONB, nullable=False, default=list)\n    conflict_alignment = Column(JSONB, nullable=False, default=list)\n    frame_logic = Column(JSONB, nullable=False, default=list)\n    nested_within = Column(JSONB, default=list)  # DEPRECATED: Use parent_id instead\n    conflicts_with = Column(JSONB, default=list)\n    logical_strain = Column(JSONB, default=list)\n\n    # STRUCTURED OBJECT FIELDS - stored as JSONB\n    narrative_tension = Column(JSONB, default=list)  # Array of {type, description}\n    activity_timeline = Column(JSONB, default=dict)  # Object with date keys\n    turning_points = Column(JSONB, default=list)  # Array of {date, description}\n    media_spike_history = Column(JSONB, default=dict)  # Object with date keys\n    source_stats = Column(JSONB, default=dict)  # Object with total_articles, sources\n    top_excerpts = Column(JSONB, default=list)  # Array of excerpt objects\n    update_status = Column(\n        JSONB, default=dict\n    )  # Object with last_updated, update_trigger\n    version_history = Column(JSONB, default=list)  # Array of version objects\n    rai_analysis = Column(JSONB, default=dict)  # RAI analysis object\n\n    # SEARCH AND PERFORMANCE FIELDS\n    narrative_embedding = Column(Vector(1536))  # For semantic similarity\n    # Note: search_vector will be added as generated column in SQL\n\n    # METADATA\n    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n\n    # Relationships\n    # Self-referential relationship for parent/child hierarchy\n    children = relationship(\n        \"NarrativeNSF1\",\n        backref=\"parent\",\n        remote_side=[id],\n        cascade=\"all, delete\",\n        passive_deletes=True,\n    )\n\n    article_associations = relationship(\n        \"NarrativeArticleAssociation\",\n        back_populates=\"narrative\",\n        cascade=\"all, delete-orphan\",\n    )\n    metrics = relationship(\n        \"NarrativeMetrics\",\n        back_populates=\"narrative\",\n        uselist=False,\n        cascade=\"all, delete-orphan\",\n    )\n\n    def __repr__(self):\n        return f\"<Narrative(id={self.id}, narrative_id='{self.narrative_id}', title='{self.title[:50]}...')>\"\n\n    def is_parent(self) -> bool:\n        \"\"\"Check if this narrative is a parent (has no parent_id).\"\"\"\n        return self.parent_id is None\n\n    def is_child(self) -> bool:\n        \"\"\"Check if this narrative is a child (has parent_id).\"\"\"\n        return self.parent_id is not None\n\n    def get_root(self):\n        \"\"\"Get the root parent narrative in the hierarchy.\"\"\"\n        if self.parent_id is None:\n            return self\n        return self.parent.get_root() if self.parent else self\n\n    def get_hierarchy_level(self) -> int:\n        \"\"\"Get the level in hierarchy (0 = parent, 1 = child).\"\"\"\n        return 0 if self.parent_id is None else 1\n\n    # ========================================================================\n    # FRINGE AND QUALITY NOTES HELPER METHODS\n    # ========================================================================\n\n    def add_fringe_note(\n        self,\n        summary: str,\n        source_count: Optional[int] = None,\n        tone: Optional[str] = None,\n        example_articles: Optional[List[str]] = None,\n    ) -> None:\n        \"\"\"Add a fringe note to track narrative outliers and low-diversity content.\n\n        Args:\n            summary: Brief 1-2 sentence description of the fringe content\n            source_count: Number of sources supporting this fringe perspective\n            tone: Tone classification (propagandistic, neutral, etc.)\n            example_articles: List of article URLs as evidence\n        \"\"\"\n        from datetime import datetime\n\n        fringe_note = {\n            \"note_type\": \"fringe\",\n            \"summary\": summary,\n            \"source_count\": source_count,\n            \"tone\": tone,\n            \"example_articles\": example_articles or [],\n            \"detected_at\": datetime.utcnow().isoformat(),\n        }\n\n        # Initialize fringe_notes if None\n        if self.fringe_notes is None:\n            self.fringe_notes = []\n\n        # Add note and update timestamp\n        self.fringe_notes = self.fringe_notes + [fringe_note]\n        self.updated_at = datetime.utcnow()\n\n    def add_data_quality_note(\n        self,\n        summary: str,\n        source_count: Optional[int] = None,\n        example_articles: Optional[List[str]] = None,\n    ) -> None:\n        \"\"\"Add a data quality note to track pipeline and processing issues.\n\n        Args:\n            summary: Brief description of the data quality issue\n            source_count: Number of sources affected by this issue\n            example_articles: List of article URLs demonstrating the issue\n        \"\"\"\n        from datetime import datetime\n\n        quality_note = {\n            \"note_type\": \"quality\",\n            \"summary\": summary,\n            \"source_count\": source_count,\n            \"example_articles\": example_articles or [],\n            \"detected_at\": datetime.utcnow().isoformat(),\n        }\n\n        # Initialize data_quality_notes if None\n        if self.data_quality_notes is None:\n            self.data_quality_notes = []\n\n        # Add note and update timestamp\n        self.data_quality_notes = self.data_quality_notes + [quality_note]\n        self.updated_at = datetime.utcnow()\n\n    def get_fringe_notes_by_tone(self, tone: str) -> List[Dict[str, Any]]:\n        \"\"\"Get all fringe notes with a specific tone.\n\n        Args:\n            tone: Target tone to filter by (e.g., 'propagandistic', 'neutral')\n\n        Returns:\n            List of fringe notes matching the tone\n        \"\"\"\n        if not self.fringe_notes:\n            return []\n\n        return [\n            note\n            for note in self.fringe_notes\n            if note.get(\"tone\") == tone and note.get(\"note_type\") == \"fringe\"\n        ]\n\n    def get_latest_quality_issues(self, limit: int = 3) -> List[Dict[str, Any]]:\n        \"\"\"Get the most recent data quality issues.\n\n        Args:\n            limit: Maximum number of issues to return\n\n        Returns:\n            List of most recent quality notes\n        \"\"\"\n        if not self.data_quality_notes:\n            return []\n\n        # Sort by detected_at timestamp descending\n        quality_notes = [\n            note\n            for note in self.data_quality_notes\n            if note.get(\"note_type\") == \"quality\"\n        ]\n\n        sorted_notes = sorted(\n            quality_notes, key=lambda x: x.get(\"detected_at\", \"\"), reverse=True\n        )\n\n        return sorted_notes[:limit]\n\n    def has_fringe_content(self) -> bool:\n        \"\"\"Check if this narrative has any fringe notes.\"\"\"\n        return bool(self.fringe_notes and len(self.fringe_notes) > 0)\n\n    def has_quality_issues(self) -> bool:\n        \"\"\"Check if this narrative has any data quality issues.\"\"\"\n        return bool(self.data_quality_notes and len(self.data_quality_notes) > 0)\n\n    def get_fringe_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary statistics about fringe content in this narrative.\n\n        Returns:\n            Dictionary with fringe analysis metrics\n        \"\"\"\n        if not self.fringe_notes:\n            return {\n                \"has_fringe\": False,\n                \"total_notes\": 0,\n                \"tones\": [],\n                \"avg_source_count\": None,\n            }\n\n        fringe_only = [n for n in self.fringe_notes if n.get(\"note_type\") == \"fringe\"]\n\n        tones = list(set(note.get(\"tone\") for note in fringe_only if note.get(\"tone\")))\n\n        source_counts = [\n            note.get(\"source_count\")\n            for note in fringe_only\n            if note.get(\"source_count\") is not None\n        ]\n\n        return {\n            \"has_fringe\": len(fringe_only) > 0,\n            \"total_notes\": len(fringe_only),\n            \"tones\": tones,\n            \"avg_source_count\": (\n                sum(source_counts) / len(source_counts) if source_counts else None\n            ),\n        }\n\n    __table_args__ = (\n        # Indexes for performance\n        Index(\"idx_narratives_narrative_id\", \"narrative_id\"),\n        Index(\"idx_narratives_origin_language\", \"origin_language\"),\n        Index(\"idx_narratives_confidence\", \"confidence_rating\"),\n        Index(\"idx_narratives_created\", \"created_at\"),\n        Index(\"idx_narratives_updated\", \"updated_at\"),\n        # Hierarchy indexes (parent_id based)\n        Index(\"idx_narratives_parent_id\", \"parent_id\"),\n        Index(\n            \"idx_narratives_parent_children\",\n            \"parent_id\",\n            postgresql_where=\"parent_id IS NOT NULL\",\n        ),\n        Index(\n            \"idx_narratives_parents\", \"parent_id\", postgresql_where=\"parent_id IS NULL\"\n        ),\n        Index(\"idx_narratives_hierarchy_created\", \"parent_id\", \"created_at\"),\n        # JSONB GIN indexes for array/object queries\n        Index(\"idx_narratives_alignment_gin\", \"alignment\", postgresql_using=\"gin\"),\n        Index(\n            \"idx_narratives_actor_origin_gin\", \"actor_origin\", postgresql_using=\"gin\"\n        ),\n        Index(\"idx_narratives_frame_logic_gin\", \"frame_logic\", postgresql_using=\"gin\"),\n        Index(\n            \"idx_narratives_nested_within_gin\", \"nested_within\", postgresql_using=\"gin\"\n        ),  # DEPRECATED\n        Index(\n            \"idx_narratives_conflicts_with_gin\",\n            \"conflicts_with\",\n            postgresql_using=\"gin\",\n        ),\n        Index(\n            \"idx_narratives_dominant_source_languages_gin\",\n            \"dominant_source_languages\",\n            postgresql_using=\"gin\",\n        ),\n        Index(\n            \"idx_narratives_conflict_alignment_gin\",\n            \"conflict_alignment\",\n            postgresql_using=\"gin\",\n        ),\n        # FRINGE AND QUALITY NOTES indexes\n        Index(\n            \"idx_narratives_fringe_notes_gin\", \"fringe_notes\", postgresql_using=\"gin\"\n        ),\n        Index(\n            \"idx_narratives_data_quality_notes_gin\",\n            \"data_quality_notes\",\n            postgresql_using=\"gin\",\n        ),\n        # Vector similarity index\n        Index(\n            \"idx_narratives_embedding\",\n            \"narrative_embedding\",\n            postgresql_using=\"ivfflat\",\n            postgresql_ops={\"narrative_embedding\": \"vector_cosine_ops\"},\n        ),\n        # Constraints\n        CheckConstraint(\n            \"confidence_rating IN ('low', 'medium', 'high', 'very_high') OR confidence_rating IS NULL\",\n            name=\"chk_narratives_confidence_rating\",\n        ),\n        CheckConstraint(\n            \"length(origin_language) = 2\", name=\"chk_narratives_origin_language_length\"\n        ),\n        CheckConstraint(\n            \"length(narrative_id) >= 3\", name=\"chk_narratives_narrative_id_length\"\n        ),\n        CheckConstraint(\"id != parent_id\", name=\"chk_narratives_no_self_reference\"),\n    )\n\n\nclass NarrativeMetrics(Base):\n    \"\"\"\n    Analytics and metrics for narratives - separate from NSF-1 content\n    This table stores all analytics, scoring, and operational data\n    while keeping NSF-1 narratives table purely for content.\n    One-to-one relationship with NarrativeNSF1.\n    \"\"\"\n\n    __tablename__ = \"narrative_metrics\"\n\n    # Foreign key to narratives table (UUID primary key)\n    narrative_uuid = Column(\n        UUID(as_uuid=True), ForeignKey(\"narratives.id\"), primary_key=True\n    )\n\n    # TEMPORAL FIELDS\n    narrative_start_date = Column(DateTime)\n    narrative_end_date = Column(DateTime)\n    last_spike = Column(DateTime)\n\n    # SCORING FIELDS\n    trending_score = Column(Float, default=0.0)\n    credibility_score = Column(Float)\n    engagement_score = Column(Float)\n    sentiment_score = Column(Float)\n\n    # PRIORITY AND STATUS\n    narrative_priority = Column(Integer, default=5)\n    narrative_status = Column(String(20), default=\"active\")\n\n    # METADATA\n    geographic_scope = Column(String(100))  # e.g., 'global', 'europe', 'us-domestic'\n    update_frequency = Column(String(50), default=\"15 minutes\")\n    version_number = Column(Integer, default=1)\n\n    # KEYWORDS FOR QUICK FILTERING\n    keywords = Column(JSON)  # Array of core tags for filtering and search\n\n    # TIMESTAMPS\n    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n\n    # Relationships\n    narrative = relationship(\"NarrativeNSF1\", back_populates=\"metrics\", uselist=False)\n\n    __table_args__ = (\n        # Indexes for dashboard and trending queries\n        Index(\"idx_narrative_metrics_trending_score\", \"trending_score\"),\n        Index(\"idx_narrative_metrics_status\", \"narrative_status\"),\n        Index(\"idx_narrative_metrics_priority\", \"narrative_priority\"),\n        Index(\"idx_narrative_metrics_credibility\", \"credibility_score\"),\n        Index(\"idx_narrative_metrics_engagement\", \"engagement_score\"),\n        Index(\"idx_narrative_metrics_start_date\", \"narrative_start_date\"),\n        Index(\"idx_narrative_metrics_geographic_scope\", \"geographic_scope\"),\n        # Composite indexes for common dashboard queries\n        Index(\n            \"idx_narrative_metrics_status_trending\",\n            \"narrative_status\",\n            \"trending_score\",\n        ),\n        Index(\n            \"idx_narrative_metrics_active_priority\",\n            \"narrative_status\",\n            \"narrative_priority\",\n        ),\n        # Constraints\n        CheckConstraint(\n            \"trending_score >= 0\", name=\"chk_narrative_metrics_trending_score\"\n        ),\n        CheckConstraint(\n            \"credibility_score >= 0 AND credibility_score <= 10 OR credibility_score IS NULL\",\n            name=\"chk_narrative_metrics_credibility_score\",\n        ),\n        CheckConstraint(\n            \"engagement_score >= 0 AND engagement_score <= 1 OR engagement_score IS NULL\",\n            name=\"chk_narrative_metrics_engagement_score\",\n        ),\n        CheckConstraint(\n            \"sentiment_score >= -1 AND sentiment_score <= 1 OR sentiment_score IS NULL\",\n            name=\"chk_narrative_metrics_sentiment_score\",\n        ),\n        CheckConstraint(\n            \"narrative_priority >= 1 AND narrative_priority <= 10\",\n            name=\"chk_narrative_metrics_priority\",\n        ),\n        CheckConstraint(\n            \"narrative_status IN ('active', 'emerging', 'declining', 'dormant', 'archived')\",\n            name=\"chk_narrative_metrics_status\",\n        ),\n        CheckConstraint(\n            \"narrative_start_date IS NULL OR narrative_end_date IS NULL OR narrative_start_date <= narrative_end_date\",\n            name=\"chk_narrative_metrics_dates\",\n        ),\n    )\n\n    def __repr__(self):\n        return f\"<NarrativeMetrics(narrative_uuid={self.narrative_uuid}, status='{self.narrative_status}', trending_score={self.trending_score})>\"\n\n\nclass NarrativeArticleAssociation(Base):\n    \"\"\"\n    Many-to-many relationship between narratives and articles\n    Links NSF-1 narratives to source articles with relevance scoring\n    \"\"\"\n\n    __tablename__ = \"narrative_articles\"\n\n    # Foreign keys\n    narrative_id = Column(\n        UUID(as_uuid=True), ForeignKey(\"narratives.id\"), primary_key=True\n    )\n    article_id = Column(UUID(as_uuid=True), ForeignKey(\"articles.id\"), primary_key=True)\n\n    # Association metadata\n    relevance_score = Column(Float, nullable=False, default=0.5)\n    assigned_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n    assigned_by = Column(String(50), default=\"auto\")  # 'auto' or 'manual'\n\n    # Relationships\n    narrative = relationship(\"NarrativeNSF1\", back_populates=\"article_associations\")\n    article = relationship(\"Article\")  # Assuming Article model exists\n\n    __table_args__ = (\n        Index(\"idx_narrative_articles_narrative_id\", \"narrative_id\"),\n        Index(\"idx_narrative_articles_article_id\", \"article_id\"),\n        Index(\"idx_narrative_articles_relevance\", \"narrative_id\", \"relevance_score\"),\n        CheckConstraint(\n            \"relevance_score >= 0 AND relevance_score <= 1\",\n            name=\"chk_narrative_articles_relevance_score\",\n        ),\n    )\n\n    def __repr__(self):\n        return f\"<NarrativeArticleAssociation(narrative_id={self.narrative_id}, article_id={self.article_id}, relevance={self.relevance_score})>\"\n\n\n# ============================================================================\n# MODEL ALIASES FOR CLEANER IMPORTS\n# ============================================================================\n\n# Provide cleaner aliases for the main models\nNarrative = NarrativeNSF1  # Main narrative model\n# NarrativeMetrics already has a clean name\n\n\n# ============================================================================\n# Database utility functions\ndef create_all_tables(engine):\n    \"\"\"Create all database tables\"\"\"\n    Base.metadata.create_all(engine)\n    logger.info(\"All database tables created successfully\")\n\n\ndef drop_all_tables(engine):\n    \"\"\"Drop all database tables (use with caution!)\"\"\"\n    Base.metadata.drop_all(engine)\n    logger.warning(\"All database tables dropped\")\n\n\ndef get_table_info():\n    \"\"\"Get information about all defined tables\"\"\"\n    tables_info = {}\n    for table_name, table in Base.metadata.tables.items():\n        tables_info[table_name] = {\n            \"columns\": [col.name for col in table.columns],\n            \"indexes\": [idx.name for idx in table.indexes],\n            \"constraints\": [const.name for const in table.constraints if const.name],\n        }\n    return tables_info\n"
}