# ðŸš€ RSS Ingestion Quick Start Guide

## âœ… **System Status: READY**

Your RSS ingestion system is **fully implemented** and ready to populate the `raw_articles` table with strategic news content.

---

## ðŸ”§ **Quick Setup**

### 1. **Install Dependencies**
```bash
pip install feedparser aiohttp langdetect beautifulsoup4 python-dateutil
```

### 2. **Test Configuration**
```bash
python test_ingestion.py
```
This will:
- âœ… Test feed configuration loading
- ðŸŒ Check connectivity to all 10 strategic feeds
- ðŸ“° Test single feed ingestion

### 3. **Start Production System**
```bash
# Terminal 1: Start Celery Worker
celery -A etl_pipeline.core.tasks.celery_app worker --loglevel=info --queues=ingestion,monitoring

# Terminal 2: Start Celery Beat (Scheduler)
celery -A etl_pipeline.core.tasks.celery_app beat --loglevel=info

# Terminal 3: Monitor (Optional)
celery -A etl_pipeline.core.tasks.celery_app flower  # Web UI at http://localhost:5555
```

---

## ðŸ“Š **What Gets Ingested**

### **Data Saved to `raw_articles` Table**:
- âœ… **URL** â†’ `url` field
- âœ… **Title** â†’ `title` field  
- âœ… **Description/snippet** â†’ `summary` field
- âœ… **Full text** â†’ `content` field (for clustering)
- âœ… **Publication date** â†’ `published_at` field
- âœ… **Source ID** â†’ `source_id` field (auto-created from config)
- âœ… **Language** â†’ `language_code` field (auto-detected)
- âœ… **Content hash** â†’ `content_hash` field (deduplication)
- âœ… **Word count** â†’ `word_count` field (quality filtering)

### **10 Strategic Feeds Configured**:
1. **Reuters World News** (with RSS generator fallback)
2. **Reuters Politics** (with RSS generator fallback)  
3. **BBC World News** (official RSS)
4. **BBC Business** (official RSS)
5. **Deutsche Welle International** (official RSS)
6. **Deutsche Welle Europe** (official RSS)
7. **POLITICO Europe** (official RSS)
8. **Al Jazeera International** (official RSS)
9. **TASS International** (official RSS)
10. **Xinhua International** (official RSS)

---

## â° **Automated Schedule**

### **Celery Beat Tasks (Auto-Running)**:
- ðŸ”„ **`etl.ingest_all_feeds`**: Every hour (3600s)
- ðŸŒ **`etl.test_feed_connectivity`**: Every 2 hours (7200s)
- ðŸ’“ **Health checks**: Every 30 minutes
- ðŸ“Š **Data quality checks**: Every hour

### **Queue Priorities**:
- **High Priority**: Orchestration (10), Real-time (10)
- **Medium Priority**: Ingestion (7), Monitoring (5-7)  
- **Low Priority**: Maintenance (1), ML Pipeline (3)

---

## ðŸŽ¯ **Manual Testing Commands**

### **Test Single Feed**:
```python
from etl_pipeline.core.tasks.ingestion_tasks import ingest_single_feed

# Test BBC World News
result = ingest_single_feed.delay("BBC World News")
print(result.get())
```

### **Test All Feeds**:
```python
from etl_pipeline.core.tasks.ingestion_tasks import ingest_all_feeds

result = ingest_all_feeds.delay()
print(result.get())
```

### **Test Connectivity**:
```python
from etl_pipeline.core.tasks.ingestion_tasks import test_feed_connectivity

result = test_feed_connectivity.delay()
print(result.get())
```

---

## ðŸ“ˆ **Quality Filtering**

### **Built-in Filters** (from `news_feeds_config.json`):
- âœ… **Min word count**: 50 words
- âœ… **Max word count**: 10,000 words  
- âœ… **Exclude keywords**: ["advertisement", "sponsored", "breaking news alert"]
- âœ… **Deduplication**: SHA-256 content hashing
- âœ… **Language detection**: Auto-detect with fallback to feed language

### **Feed Reliability Scoring**:
- **High reliability**: Reuters, BBC, DW (score: 8.0)
- **Medium reliability**: POLITICO, Al Jazeera (score: 6.0)
- **Government sources**: TASS, Xinhua (score: 6.0)

---

## ðŸ” **Monitoring & Debugging**

### **Check Database**:
```sql
-- Count articles by source
SELECT ns.source_name, COUNT(*) as article_count
FROM raw_articles ra
JOIN news_sources ns ON ra.source_id = ns.source_id
WHERE ra.scraped_at >= NOW() - INTERVAL '24 hours'
GROUP BY ns.source_name
ORDER BY article_count DESC;

-- Recent articles
SELECT title, source_name, published_at, word_count
FROM raw_articles ra
JOIN news_sources ns ON ra.source_id = ns.source_id
ORDER BY ra.scraped_at DESC
LIMIT 10;
```

### **Check Celery Status**:
```bash
# Active tasks
celery -A etl_pipeline.core.tasks.celery_app inspect active

# Scheduled tasks  
celery -A etl_pipeline.core.tasks.celery_app inspect scheduled

# Worker stats
celery -A etl_pipeline.core.tasks.celery_app inspect stats
```

### **Logs Location**:
- **Celery Worker**: stdout/stderr or configured log file
- **Application Logs**: Structured JSON logs via `structlog`
- **Database Logs**: PostgreSQL logs for query performance

---

## ðŸš¨ **Troubleshooting**

### **Common Issues**:

**1. RSS Feed Connection Errors**:
```bash
# Test individual feed
python test_ingestion.py
# Check feed_connectivity task results
```

**2. Database Connection Issues**:
```bash
# Check PostgreSQL connection
psql -h localhost -U your_user -d narrative_intelligence -c "SELECT 1;"
```

**3. Redis Connection Issues**:  
```bash
# Check Redis connection
redis-cli ping
```

**4. Language Detection Errors**:
```bash
# Install language detection
pip install langdetect
```

**5. Missing Dependencies**:
```bash
# Install all ingestion requirements
pip install -r requirements_ingestion.txt
```

---

## ðŸŽ¯ **Success Metrics**

### **Expected Performance**:
- **Ingestion Rate**: 500-1000 articles/hour across all feeds
- **Success Rate**: >95% for reliable feeds (BBC, DW, POLITICO)
- **Deduplication**: <5% duplicate rate
- **Language Detection**: >90% accuracy
- **Feed Uptime**: >98% for official RSS feeds

### **Ready for Next Phase**: 
âœ… Articles flowing into `raw_articles`  
âœ… Deduplication working  
âœ… Quality filtering active  
âœ… **READY FOR SEMANTIC CLUSTERING (CLUST-1)** ðŸš€

---

**System Status**: ðŸŸ¢ **PRODUCTION READY**  
**Next Milestone**: Implement semantic clustering for narrative generation