My notes:
We want to analyse the bulk of titles to extract real vocab for our taxonomy and build a comprehensive filter.
This is a one-off Project that we Need to impement initially. It is not a part of the Pipeline and thus a Major distraction.
Both Grok and DeepSeek confirmed the solution by GPT. And proposed some tweaks. 
I am going to work on it separately. Right now the Focus is on the Pipeline.
 
Totally with you. The fix isn’t “50 new labels,” it’s **richer anchors + better aliases** learned from your own corpus. Here’s a tight, one-time *offline* taxonomy-build plan that keeps the pipeline simple but makes the gate much smarter.

# Plan: grow vocab from \~10k titles (offline, once)

## A) Mechanisms — keep labels small, expand anchors big

**Principle:** freeze the **label set** (Core-20 ± a few), but grow each label’s **anchor phrases** to dozens/hundreds, mined from real headlines and across languages. This boosts recall without adding moving parts.

**Steps (one script per step; all local, not part of runtime pipeline):**

1. **Export corpus**

* Dump `title_norm, lang, publisher_domain, pubdate_utc` for \~10k items.
* Include both `gate_keep=true/false` so we see what we’re missing.

2. **Phrase mining (language-agnostic first)**

* Build n-grams (uni/bi/tri) with freq thresholds by language.
* Keep n-grams containing verby tokens or policy/action words (simple lists per lang; start EN + RU + ZH; others fall back to frequency).
* Optional: simple POS with small spaCy models for EN/ES/FR/DE; skip POS for RU/ZH (use frequency + patterns).

3. **Seeded similarity expansion**

* For each mechanism label (seed anchors already in `mechanisms.json`):

  * Encode seeds + all candidate n-grams with **all-MiniLM-L6-v2** (cross-lingual).
  * Compute cosine to each seed; take **max** over a label’s seeds.
  * Keep candidates with sim ≥ **0.65–0.70**; cap **≤ 100** per label; dedupe by lowercase.
  * Auto-tag candidate language (from n-gram origin) so you keep multilingual variants.

4. **Precision guardrails**

* Drop phrases that match generic fluff (e.g., “warns”, “says”) unless they co-occur with a mechanism keyword (“sanctions”, “strike”, “bans”).
* For **guarded mechanisms** (sanctions/export\_controls/strike\_airstrike/platform\_ban), enforce a must-contain list (e.g., “sanction|санкци|制裁”, “strike|airstrike|missile|ракетн|导弹”, “ban|block|封禁”).

5. **Human skim (30–45 min)**

* For each label, sort by score desc; keep the top **\~40–80** anchors; remove obvious junk. This is the only manual step.

6. **Write back `mechanisms.json` v2**

* Same schema, just bigger `anchors_en/…` lists (or a single multilingual `anchors` array).
* Add `version: "2025-09-xx"` inside the JSON.

7. **Evaluate**

* Re-run gate on a held-out 1k titles.
* Report: keep-rate Δ, precision sample (label 100 kept/50 dropped), top “newly kept” examples.
* Accept if: (+) recall increases on strategic items, (≈) precision doesn’t tank (target ≥85% on a 100-sample spot check).

**Why not 300–500 labels?** You don’t need them. Most variance is in *phrasing*, not categories. Keep ≤30 labels; make anchors rich.

---

## B) Actors — keep your 60 canon, mine better aliases

**Principle:** don’t add actors casually; **expand aliases** per actor per language.

**Steps:**

1. **Co-occurrence mining**

* For each canonical actor (e.g., `US`), gather titles that already matched existing aliases.
* Extract capitalized tokens, demonyms, local endonyms, common shorthand near mentions (±3 tokens window).
* Keep candidates by frequency (≥5) per language.

2. **Cross-lingual confirmation**

* Encode candidates and existing alias set; keep new ones with cosine ≥ **0.70** to any known alias phrase, or that appear on **known domains** (e.g., `*.gov.cn` → likely “中方/中国”).
* Quick regex whitelist for RU (`Россия|РФ|Кремль`), ZH (`中国|中方|美方|俄方|台方`), AR etc., just a few per language.

3. **Light review (20–30 min)**

* Add ≤5 new aliases per actor max to avoid bloat.

4. **Write back `actors.csv` v2**

* Append new semicolon-separated aliases in the right language columns.
* Bump a `version` comment line at the top (or keep version in a sidecar JSON).

5. **Evaluate**

* Re-run the gate; compare “actor\_hit” share across languages. Spot-check 50 random actor hits for false positives.

---

## C) What changes in the code? (minimal)

* No schema changes.
* Replace the files under `data/vocab/actors.csv` and `data/vocab/mechanisms.json` with the **expanded** versions.
* No code edits in `strategic_gate.py` beyond bumping the config `mechanisms_json_path` version if you keep both files.
* Add two **offline** scripts under `scripts/`:

  * `scripts/taxonomy/mine_mechanism_anchors.py` (does A.2–A.6)
  * `scripts/taxonomy/mine_actor_aliases.py` (does B.1–B.4)
* Add a **sanity test**: `tests/clust1/test_gate_regression.py` that loads the new vocab and asserts:

  * Known strategic examples now pass the gate.
  * Non-strategic (sports/celebs) remain out.

---

## D) Acceptance criteria (for this one-time build)

* Mechanism anchors per label: **≥40** avg, covering EN + RU + ZH (others opportunistic).
* Actor aliases: **+2–5** new per top 20 actors; RU/ZH coverage improved.
* Gate metrics on a 1k sample:

  * Keep-rate increases by **+10–20 pp** vs. old vocab on clearly strategic categories (geo/defense/diplomacy/energy).
  * Spot-check precision ≥ **85%** on kept; obvious non-strategic items (sports/celebs) still rejected.
* No runtime code changes required beyond loading the new files.

---

## E) Notes on models/tools

* **all-MiniLM-L6-v2** *is* cross-lingual; that’s why we can compare EN/RU/ZH phrases in one vector space.
* If you later want even better multilingual, you can swap in `paraphrase-multilingual-MiniLM-L12-v2` with the same code paths; keep it a config switch.

---

## F) Recommended next tiny tweaks (after vocab refresh)

* Add a **People** mini-set (e.g., `TRUMP`, `PUTIN`, `XI`, `MODI`, `NETANYAHU`) as *actors of interest* (kind=`person`) used **only** to *boost* gate score, never to guard issuer→target. Keeps “Trump climate satellites” in scope without breaking state-level discipline.
* Add **two new mechanism labels** only if data shows big unclassified clusters:

  * `budget_appropriation` (“spending bill”, “appropriations”, “funding vote”)
  * `industrial_policy` (“subsidy package”, “production incentives”, “nationalization/privatization”)

Both are common in strategic headlines and reduce “unspecified”.

---

If you want, I can draft the two offline mining scripts (with comments) so Claude can drop them into `scripts/taxonomy/` and run them on your 10k dump.

---

Short answer: **save Claude’s file as-is, then ask for a tiny patch PR with the deltas.**
That keeps scope tight and avoids the “rewrite drift” you’re worried about.

Here’s a clean way to proceed.

# Recommended flow

1. **Create the file now**
   Confirm the current `apps/clust1/strategic_gate.py`. Commit it.

2. **Open a small “delta” ticket + patch PR**
   Ask Claude to *modify the existing file* with surgical changes, not to rewrite it.

3. **Run the same smoke tests**
   After the patch, re-run the existing tests to prove no regressions.

# Paste this to Claude Code (verbatim)

> **Task:** Apply a small patch to `apps/clust1/strategic_gate.py`. Do *not* rewrite the file. Keep public API identical.
>
> **Changes (surgical):**
>
> 1. **Actor pattern tuple carries the actual alias**
>
>    * In `_compile_actor_patterns()`, store `(entity_id, alias_lower, pattern_or_none, use_substring)` so `_actor_match()` checks only that alias (no re-lookup of all aliases for the entity).
> 2. **CJK/one-word alias logic**
>
>    * Keep current CJK substring rule, but use the stored `alias_lower` instead of scanning the whole alias list again.
> 3. **Threshold clamp + config guard**
>
>    * Clamp `cosine_threshold_gate` into `[0.0, 0.99]` once at init; raise if missing/NaN.
>    * If no valid anchor phrases after loading mechanisms, raise a clear `ValueError` (message stays the same).
> 4. **Empty/short text fast-path**
>
>    * In `filter_title()` return `GateResult(False, 0.0, "below_threshold", [], [])` when normalized text length `< 3`.
> 5. **Batch index safety**
>
>    * In the batch path, derive `similarities` by index lookup only for items that were encoded; don’t increment `batch_idx` for actor-hit/empty texts.
> 6. **Docstring + type nits**
>
>    * Add a short docstring to `_has_cjk_chars`.
>    * Ensure all private helpers return concrete `Tuple[...]` types (no implicit).
>
> **Do not change:**
>
> * File location, class names, public method signatures, or `GateResult` fields.
> * Externalized vocab loading (`load_actor_aliases`, `load_mechanism_anchors`) and config keys.
>
> **After patch:**
>
> * Run existing tests in `tests/clust1/test_gate_smoke.py` and show results.
> * Print a one-line summary: “StrategicGate patch: patterns carry alias ✓, threshold clamped ✓, batch index safe ✓”.

# Why this approach

* Minimizes churn: you keep a known-good baseline and add **just** the fixes.
* Prevents the “LLM rewrote everything” problem.
* Your current tests keep guarding the behavior.




